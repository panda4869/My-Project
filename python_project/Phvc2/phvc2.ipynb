{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import sys\n",
    "#sys.path.append(\"../..\")\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings \n",
    "import operator\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('traindata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    39601.000000\n",
       "mean         0.171334\n",
       "std          0.376805\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: couple1, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['couple1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38850.000000\n",
       "mean         0.170991\n",
       "std          0.376506\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: couple1, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_preprocess['couple1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_preprocess,train_data_user_id=data_preprocession('traindata.csv',Train=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5668\n",
       "1    9094\n",
       "2    5999\n",
       "3    5592\n",
       "4    7133\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_user_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(filename,Train):\n",
    "    data=pd.read_csv(os.path.join(os.getcwd(),filename))\n",
    "    \n",
    "    ### categorize the variable\n",
    "    def cat_booking_hour(x):\n",
    "        return 'On_Demand' if x <= 3 else \"Adv_Booked\"\n",
    "    \n",
    "    def booking_interval(x):\n",
    "        if x<0:\n",
    "            return 0\n",
    "        elif x<=180:\n",
    "            return x\n",
    "        else:\n",
    "            return 180\n",
    "    \n",
    "\n",
    "    def cat_client_rank(x):\n",
    "        return 1 if x in [\"AAA\", \"AA\", \"A\"] else 0\n",
    "\n",
    "    def cat_city_maturity(x):\n",
    "        if x in [2,5,3,4,13,12,9,1,14,15,17,18,10,16]:\n",
    "            return 'Grandfather_Market'\n",
    "        elif x in [20,22,24,25,21,23,27,29,30,31,32,34,53,35]:\n",
    "            return 'Mature_Market'\n",
    "        elif x in [57,56,58,38,28,33,36,40,45,37,41,52,46,43]:\n",
    "            return 'Medium_Age_Market'\n",
    "        else:\n",
    "            return 'New_Market'\n",
    "\n",
    "    def signtobook(x):\n",
    "        if x <=0 :\n",
    "            return 0\n",
    "        elif x>=90:\n",
    "            return 90\n",
    "        else:\n",
    "            return x\n",
    "    ### categorical transformation \n",
    "\n",
    "    if Train:\n",
    "        data['client_rank_label'] = data['rank'].apply(cat_client_rank)\n",
    "\n",
    "    data['booking_hour_ahead1'] = data['hours_ahead1'].apply(cat_booking_hour)\n",
    "    data['booking_hour_ahead2'] = data['hours_ahead2'].apply(cat_booking_hour)\n",
    "    data['market_maturity'] = data['city_id'].apply(cat_city_maturity)\n",
    "    data['signup_to_book_interval'] = data['sign_up_to_book'].apply(signtobook)\n",
    "    data['booking_interval']=data['interval_between_bookings'].apply(booking_interval)\n",
    "    #data['session_length_type'] = data['session_length'].apply(cat_session_length)\n",
    "    data['income_level'] = data.median_income\n",
    "    ### drop columns\n",
    "    column_drop = ['user_id', 'sign_up_to_book', 'hours_ahead1','hours_ahead2',  'city_id', 'median_income',\\\n",
    "                   'rank','interval_between_bookings']\n",
    "    data_merge = data.drop(column_drop, axis = 1)\n",
    "    ### create dummy variable\n",
    "    column_dummy = ['booking_hour_ahead1','booking_hour_ahead2', 'market_maturity']\n",
    "    data_train_dummies = pd.get_dummies(data_merge, columns = column_dummy,drop_first=True)\n",
    "    ### make categorical variable n-1\n",
    "    #baseline = ['booking_hour_ahead_Adv_Booked','market_maturity_New_Market','session_length_type_short_session']\n",
    "    #data_train_dummies_remove_base = data_train_dummies.drop(baseline, axis = 1)\n",
    "    return data_train_dummies,data['user_id']\n",
    "\n",
    "### data scaling\n",
    "def min_max_scale(train_data, test_data):\n",
    "    sc = MinMaxScaler()\n",
    "    scale_list = ['signup_to_book_interval','booking_interval','quality_score1','quality_score2','quality_score_diff',\\\n",
    "                  'revenue1','revenue2',\\\n",
    "                  'income_level','num_hvc']\n",
    "    data_train_dummies_remove_base_scaled_train = train_data.loc[:,[s for s in train_data.columns if s not in scale_list]]\n",
    "    data_train_dummies_remove_base_scaled_test = test_data.loc[:,[s for s in test_data.columns if s not in scale_list]]\n",
    "    for candidate_column in scale_list:\n",
    "        data_train_dummies_remove_base_scaled_train[candidate_column] = sc.fit_transform(train_data.loc[:,candidate_column])  \n",
    "        data_train_dummies_remove_base_scaled_test[candidate_column] = sc.transform(test_data.loc[:,candidate_column])\n",
    "    return data_train_dummies_remove_base_scaled_train,data_train_dummies_remove_base_scaled_test\n",
    "\n",
    "### model fit\n",
    "\n",
    "def get_params(model, param_list):\n",
    "    model_name = model.__class__.__name__\n",
    "    params = param_list.get(model_name, {})\n",
    "    return params\n",
    "\n",
    "\n",
    "def model_fit(model, init, param_list, X_train, y_train):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    if init:\n",
    "        params = get_params(model,param_list)\n",
    "        model.set_params(**params)\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "def auc_cross_validate(model,X_train,y_train):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(-1)\n",
    "    skf = StratifiedKFold(n_splits = 3, random_state = 1234, shuffle = False)\n",
    "    score = []\n",
    "    for train_index,test_index in skf.split(X_train,y_train):\n",
    "        train_data_x,test_data_x = X_train[train_index],X_train[test_index]\n",
    "        train_data_y,test_data_y = y_train[train_index],y_train[test_index]\n",
    "        model.fit(train_data_x,train_data_y)\n",
    "        pred = model.predict_proba(test_data_x)[:,1]\n",
    "        score.append(metrics.roc_auc_score(test_data_y,pred))\n",
    "    return (np.mean(score))\n",
    "    \n",
    "\n",
    "\n",
    "def find_best_params(model,X_train,y_train,PARAM_GRID,cv,scoring,verbose):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(-1)\n",
    "    gv = GridSearchCV(model, PARAM_GRID[model.__class__.__name__], cv = cv, scoring = scoring, verbose = verbose)\n",
    "    gv.fit(X_train,y_train)\n",
    "    params = model.get_params()\n",
    "    params.update(gv.best_params_)\n",
    "    model.set_params(**params)\n",
    "    auc = auc_cross_validate(model,X_train,y_train)\n",
    "    return params,auc\n",
    "\n",
    "### save and load model\n",
    "\n",
    "def save_model(model):\n",
    "    model_name = model.__class__.__name__\n",
    "    save_path = os.getcwd()+'/'+model_name+'.pkl'\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    outfile = open(save_path,'wb')\n",
    "    try:\n",
    "        pickle.dump(model,outfile)\n",
    "        outfile.close()\n",
    "    except Exception as e:\n",
    "        print(\"Error in save_model\")\n",
    "        print(e)\n",
    "        #newrelic.agent.record_exception(e)\n",
    "\n",
    "    #s3_connection.upload_file(file_name = model_name+\".pkl\")\n",
    "    \n",
    "def save_pred(filename,preds):\n",
    "    save_path=os.path.join(os.getcwd(),filename+'.pkl')\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    outfile=open(save_path,'wb')\n",
    "    try:\n",
    "        pickle.dump(preds,outfile)\n",
    "        outfile.close()\n",
    "    except Exception as e:\n",
    "        print('Error in save_pred')\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "def current_model(**save_path):\n",
    "    try:\n",
    "        if save_path:\n",
    "            pass\n",
    "        else:\n",
    "            save_path = os.getcwd()\n",
    "        for file in os.listdir(save_path):\n",
    "            if file.endswith('.pkl'):\n",
    "                print (file)\n",
    "    except:\n",
    "        print ('Invaid Path')\n",
    "\n",
    "def load_model(model_name):\n",
    "    try:\n",
    "        #s3_connection.download_file(model_name+\".pkl\")\n",
    "        load_path = os.getcwd()+'/'+model_name+'.pkl'\n",
    "        infile = open(load_path,'rb')\n",
    "        model = pickle.load(infile)\n",
    "        infile.close()\n",
    "        print('Model Loaded Succesfully.')\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(\"Error in load_model\")\n",
    "        print(e)\n",
    "        #newrelic.agent.record_exception(e)\n",
    "\n",
    "def save_threshold(threshold_dict):\n",
    "    model_name = \"threshold_dict.pkl\"\n",
    "    save_path = os.getcwd()+'/' + model_name\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "        print('Old File Removed.')\n",
    "    \n",
    "    try:\n",
    "        outfile = open(save_path,'wb')\n",
    "        pickle.dump(threshold_dict,outfile)\n",
    "        outfile.close()\n",
    "        print ('Saved Threshold Dict Successfully.')\n",
    "        #s3_connection.upload_file(file_name = model_name)\n",
    "    except:\n",
    "        print('Failed to Save the Threshold Dict.')\n",
    "\n",
    "def load_threshold():\n",
    "    pickle_file = \"threshold_dict.pkl\"\n",
    "    try:\n",
    "        #s3_connection.download_file(pickle_file)\n",
    "        load_path = os.getcwd()+'/' + pickle_file\n",
    "        infile = open(load_path,'rb')\n",
    "        threshold_dict = pickle.load(infile)\n",
    "        infile.close()\n",
    "        print('Threshold Dict Loaded Succesfully.')\n",
    "        return threshold_dict\n",
    "    except Exception as e:\n",
    "        print(\"Error in load_threshold\")\n",
    "        print(e)\n",
    "\n",
    "\n",
    "### select best threshold\n",
    "\n",
    "def best_threshold_cv(model,X_train,y_train):\n",
    "    best_threshold = 0\n",
    "    best_f1_score = -1\n",
    "    best_recall = 0\n",
    "    best_accuracy_score = 0\n",
    "    skf = StratifiedKFold(n_splits=3,random_state=1234,shuffle=False)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(-1)    \n",
    "\n",
    "    for threshold in np.arange(0.001,0.999,0.001):\n",
    "        f1_score = []\n",
    "        recall_score = []\n",
    "        accuracy_score = []\n",
    "        for train_index,test_index in skf.split(X_train,y_train):\n",
    "            test_data_x = X_train[test_index]\n",
    "            test_data_y = y_train[test_index]\n",
    "            p1 = np.where(model.predict_proba(test_data_x)[:,1]>=threshold,1,0)\n",
    "            f1_score.append(metrics.f1_score(test_data_y,p1))\n",
    "            recall_score.append(metrics.recall_score(test_data_y,p1))\n",
    "            accuracy_score.append(metrics.accuracy_score(test_data_y,p1))\n",
    "        current_score = np.mean(f1_score)\n",
    "        current_recall = np.mean(recall_score)\n",
    "        current_accuracy_score = np.mean(accuracy_score)\n",
    "        if (current_score >= best_f1_score) & (current_recall >= 0.60):\n",
    "            best_f1_score = current_score\n",
    "            best_threshold = threshold\n",
    "            best_recall = current_recall\n",
    "            best_accuracy_score = current_accuracy_score\n",
    "\n",
    "    return best_threshold,best_accuracy_score,best_f1_score,best_recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_preprocess.to_csv('processed_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_Train=train_data_preprocess.loc[:,train_data_preprocess.columns!='client_rank_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_Train=train_data_preprocess.loc[:,train_data_preprocess.columns=='client_rank_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Random_State=1234\n",
    "x_train_not_scaled,x_test_not_scaled,y_train,y_test=train_test_split(x_Train,y_Train,test_size=0.2,random_state=Random_State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train,x_test=min_max_scale(x_train_not_scaled,x_test_not_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_lr=LogisticRegression(penalty='l1',random_state=Random_State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77038942037\n"
     ]
    }
   ],
   "source": [
    "print(auc_cross_validate(model=clf_lr,X_train=x_train,y_train=y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.159, 0.76306818181818181, 0.33232101191522345, 0.5770775409329626)\n"
     ]
    }
   ],
   "source": [
    "print(best_threshold_cv(clf_lr,x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DecisionTreeClassifier',\n",
       " 'ExtraTreesClassifier',\n",
       " 'XGBClassifier',\n",
       " 'RandomForestClassifier',\n",
       " 'LogisticRegression']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(INIT_GRID.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AdaBoostClassifier:{'learning_rate':0.1,'random_state':Random_State}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={'a':1,'b':2,'c':3}\n",
    "sorted(a.items(),key=operator.itemgetter(1),reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_filename='traindata.csv'\n",
    "test_filename='testdata.csv'\n",
    "Random_State=12345\n",
    "# initiate model \n",
    "INIT_GRID={'LogisticRegression':{'C':0.1,'penalty':'l1','class_weight': 'balanced'},\n",
    "       'RandomForestClassifier':{'n_estimators':500,'max_depth':10,'max_features':.1,'min_samples_leaf':2,'random_state':Random_State},\n",
    "        'XGBClassifier':{'base_score': 0.5,'colsample_bylevel': 1,'colsample_bytree': 1,'gamma': 0,'learning_rate': 0.1,'max_delta_step': 0,'max_depth': 3,'min_child_weight': 1,'missing': None,'n_estimators': 10,'nthread': -1,\n",
    "        'objective': 'binary:logistic','reg_alpha': 0,'reg_lambda': 1,'scale_pos_weight': 1,'seed': 0,'silent': True,'subsample': 1},\n",
    "        'ExtraTreesClassifier':{'n_estimators':500,'max_depth':10,'max_features':.1,'min_samples_leaf':10,'random_state':Random_State},\n",
    "        'DecisionTreeClassifier': {'max_features':0.8,'max_depth':10,'min_samples_leaf':10,'random_state':Random_State},\n",
    "        'AdaBoostClassifier':{'learning_rate':0.1,'random_state':Random_State}\n",
    "       }\n",
    "\n",
    "PARAM_GRID={'LogisticRegression':{'C':[0.0001,0.001,0.01,0.1],'class_weight': ['balanced',None],'penalty':['l1','l2']},\n",
    "       'RandomForestClassifier':{'max_depth':[3,5,10],'max_features':['auto','sqrt'],'min_samples_leaf':[2,4,6,8]},\n",
    "        'XGBClassifier':{'max_depth':[3,5,10],'learning_rate':[0.01,0.1,0.5,1],'min_child_weight':range(1,6,2),'gamma':[i/10.0 for i in range(0,5)]},\n",
    "        'ExtraTreesClassifier':{'max_depth':[3,5,10],'max_features':['auto','sqrt'],'min_samples_leaf':[2,4,6,8]},\n",
    "        'DecisionTreeClassifier': {'max_features':['auto','sqrt'],'max_depth':[3,5,10],'min_samples_leaf':[2,4,6,8]},   \n",
    "        'AdaBoostClassifier':{'learning_rate':[0.01,0.1,1.],'n_estimators':[10,50,100]}\n",
    "       }\n",
    "\n",
    "# preprocess the training set\n",
    "train_data_preprocess,train_data_user_id=data_preprocessing(train_filename,Train=True)\n",
    "x_Train_not_scaled=train_data_preprocess.loc[:,train_data_preprocess.columns!='client_rank_label']\n",
    "y_Train=train_data_preprocess.loc[:,train_data_preprocess.columns=='client_rank_label']\n",
    "x_train_not_scaled,x_test_not_scaled,y_train,y_test=train_test_split(x_Train_not_scaled,y_Train,test_size=0.2,random_state=Random_State)\n",
    "x_train,x_test=min_max_scale(x_train_not_scaled,x_test_not_scaled)\n",
    "\n",
    "# preprocess the testing set\n",
    "test_data_preprocess,test_data_user_id=data_preprocessing(test_filename,Train=False)\n",
    "x_Test_not_scaled=test_data_preprocess.loc[:,test_data_preprocess.columns!='client_rank_label']\n",
    "x_Train,x_Test=min_max_scale(train_data=x_Train_not_scaled,test_data=x_Test_not_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_cv_preds(model,X_train,y_train,random_state):\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train).reshape(-1)\n",
    "    model_name=model.__class__.__name__\n",
    "    clf=load_model(model_name)\n",
    "    skf=StratifiedKFold(n_splits=3,random_state=random_state,shuffle=False)\n",
    "    stack_preds=list()\n",
    "    cv_index=list()\n",
    "    for train_index,test_index in skf.split(X_train,y_train):\n",
    "        train_data_x,test_data_x=X_train[train_index],X_train[test_index]\n",
    "        train_data_y,test_data_y=y_train[train_index],y_train[test_index]\n",
    "        clf.fit(train_data_x,train_data_y)\n",
    "        stack_preds.extend(list(clf.predict_proba(test_data_x)[:,1]))\n",
    "        #stack_preds.extend(list(test_data_y))\n",
    "        cv_index.extend(test_index)\n",
    "    stack_preds=np.array(stack_preds)[sp.argsort(cv_index)]\n",
    "    return stack_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Succesfully.\n"
     ]
    }
   ],
   "source": [
    "stack_preds=get_model_cv_preds(XGBClassifier(),X_train=x_train,y_train=y_train,random_state=Random_State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83155759053596423"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_train,stack_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stacking(models,X_train,y_train,X_test,test_user_id,random_state,train,prediction,PARAM_GRID):\n",
    "    if train:\n",
    "        generalizer=LogisticRegression()\n",
    "        stage0_train=list()\n",
    "        #stage0_predict=list()\n",
    "        for model in models:\n",
    "            #model.fit(np.array(X_train),np.array(y_train).reshape(-1))\n",
    "            #stage0_predict.append(model.predict_proba(np.array(X_train))[:,1])\n",
    "            stage0_train.append(get_model_cv_preds(model,X_train,y_train,random_state))\n",
    "        skf=StratifiedKFold(n_splits=3,shuffle=False,random_state=Random_State)\n",
    "        best_g_params,best_g_auc=find_best_params(model=generalizer,cv=skf,PARAM_GRID=PARAM_GRID,scoring='roc_auc',verbose=1,X_train=np.array(stage0_train).T,y_train=y_train)\n",
    "        print('Best Generalizer Parms: ',best_g_params)\n",
    "        print('Best Generalizer AUC: ',best_g_auc)\n",
    "        generalizer.set_params(**best_g_params)\n",
    "        generalizer.fit(np.array(stage0_train).T,np.array(y_train).reshape(-1))\n",
    "        save_model(generalizer)\n",
    "        print('''\n",
    "        ###################################\n",
    "        Finished Training the Stacked Model\n",
    "        ###################################''')\n",
    "        \n",
    "        best_threshold,best_accuracy_score,best_f1_score,best_recall=best_threshold_cv(generalizer,X_train=np.array(stage0_train).T,y_train=np.array(y_train).reshape(-1))\n",
    "        threshold_dict={'Best_Threshold':best_threshold,'Best_Accuracy_Score':best_accuracy_score,'Best_F1_Score':best_f1_score,'Best_Recall':best_recall}\n",
    "        print('Best Threshold: ',best_threshold,'Best Accuarcy Score: ',best_accuracy_score,'Best F1 Score: ',best_f1_score,'Best Recall: ',best_recall)\n",
    "        save_threshold(threshold_dict)\n",
    "        print('''\n",
    "        ###############################\n",
    "        Finished Training the Threshold\n",
    "        ###############################''')\n",
    "    if prediction:\n",
    "        generalizer=load_model('LogisticRegression')\n",
    "        threshold_dict=load_threshold()\n",
    "        print('Best Threshold: ', threshold_dict['Best_Threshold'])\n",
    "        stage1_test=list()\n",
    "        for model in models:\n",
    "            model_pred=load_model(model.__class__.__name__)\n",
    "            model_pred.fit(np.array(X_train),np.array(y_train).reshape(-1))\n",
    "            stage1_test.append(list(model_pred.predict_proba(np.array(X_test))[:,1]))\n",
    "        \n",
    "        #return generalizer.predict_proba(np.array(stage1_test).T)[1:]\n",
    "        print (np.array(stage1_test).shape)\n",
    "        prob=generalizer.predict_proba(np.array(stage1_test).T)[:,1]\n",
    "        preds=np.where(prob>=threshold_dict['Best_Threshold'],1,0)\n",
    "        output=pd.DataFrame({'user_id':test_user_id,'score':prob,'status':preds},columns=['user_id','score','status'])\n",
    "        if os.path.exists('predict.csv'):\n",
    "            os.remove('predict.csv')\n",
    "        output.to_csv('predict.csv',index=False)\n",
    "        print('''\n",
    "        ########################\n",
    "        Done running prediction\n",
    "        ########################''')\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Succesfully.\n",
      "Model Loaded Succesfully.\n",
      "Model Loaded Succesfully.\n",
      "Model Loaded Succesfully.\n",
      "Model Loaded Succesfully.\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:    9.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Generalizer Parms:  {'fit_intercept': True, 'multi_class': 'ovr', 'tol': 0.0001, 'dual': False, 'n_jobs': 1, 'solver': 'liblinear', 'verbose': 0, 'class_weight': 'balanced', 'max_iter': 100, 'random_state': None, 'intercept_scaling': 1, 'penalty': 'l1', 'warm_start': False, 'C': 0.1}\n",
      "Best Generalizer AUC:  0.833793107172\n",
      "\n",
      "        ###################################\n",
      "        Finished Training the Stacked Model\n",
      "        ###################################\n",
      "Best Threshold:  0.614 Best Accuarcy Score:  0.814235927655 Best F1 Score:  0.400662332699 Best Recall:  0.60281462516\n",
      "Old File Removed.\n",
      "Saved Threshold Dict Successfully.\n",
      "\n",
      "        ###############################\n",
      "        Finished Training the Threshold\n",
      "        ###############################\n",
      "Model Loaded Succesfully.\n",
      "Threshold Dict Loaded Succesfully.\n",
      "Best Threshold:  0.614\n",
      "Model Loaded Succesfully.\n",
      "Model Loaded Succesfully.\n",
      "Model Loaded Succesfully.\n",
      "Model Loaded Succesfully.\n",
      "Model Loaded Succesfully.\n",
      "(5, 305)\n",
      "\n",
      "        ########################\n",
      "        Done running prediction\n",
      "        ########################\n"
     ]
    }
   ],
   "source": [
    "stacking(models=models,X_train=x_train,y_train=y_train,X_test=x_Test,test_user_id=test_data_user_id,PARAM_GRID=PARAM_GRID,train=True,prediction=True,random_state=Random_State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4289, 26)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prime_session1', 'prime_session2', 'late_session1', 'late_session2']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in x_train.columns if x not in x_Test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_pred('stage0_train',stack_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_model_list=['RandomForestClassifier','XGBClassifier','ExtraTreesClassifier','DecisionTreeClassifier','AdaBoostClassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models=[RandomForestClassifier(),XGBClassifier(),ExtraTreesClassifier(),DecisionTreeClassifier(),AdaBoostClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stacking() missing 1 required positional argument: 'test_user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-d0b33aea9db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstacking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPARAM_GRID\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPARAM_GRID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRandom_State\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: stacking() missing 1 required positional argument: 'test_user_id'"
     ]
    }
   ],
   "source": [
    "stacking(models=models,PARAM_GRID=PARAM_GRID,train=True,test_user_id=[],prediction=False,X_test=[],X_train=x_train,y_train=y_train,random_state=Random_State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['diff_zip', 'quality_score1', 'quality_score2', 'quality_score_diff',\n",
       "       'revenue1', 'revenue2', 'discount_ratio1', 'discount_ratio2',\n",
       "       'prime_session1', 'prime_session2', 'late_session1', 'late_session2',\n",
       "       'couple1', 'couple2', 'weekend1', 'weekend2', 'special_request2',\n",
       "       'num_hvc', 'client_rank_label', 'signup_to_book_interval',\n",
       "       'booking_interval', 'income_level', 'booking_hour_ahead1_On_Demand',\n",
       "       'booking_hour_ahead2_On_Demand', 'market_maturity_Mature_Market',\n",
       "       'market_maturity_Medium_Age_Market'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_preprocess.iloc[:,:-1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['diff_zip', 'quality_score1', 'quality_score2', 'quality_score_diff',\n",
       "       'revenue1', 'revenue2', 'discount_ratio1', 'discount_ratio2',\n",
       "       'prime_session1', 'prime_session2', 'late_session1', 'late_session2',\n",
       "       'couple1', 'couple2', 'weekend1', 'weekend2', 'special_request2',\n",
       "       'num_hvc', 'client_rank_label', 'signup_to_book_interval',\n",
       "       'booking_interval', 'income_level', 'booking_hour_ahead1_On_Demand',\n",
       "       'booking_hour_ahead2_On_Demand', 'market_maturity_Mature_Market',\n",
       "       'market_maturity_Medium_Age_Market', 'market_maturity_New_Market'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_preprocess.iloc[:,:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method add_argument in module argparse:\n",
      "\n",
      "add_argument(*args, **kwargs) method of argparse.ArgumentParser instance\n",
      "    add_argument(dest, ..., name=value, ...)\n",
      "    add_argument(option_string, option_string, ..., name=value, ...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "help(parser.add_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 0.00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time=time.time()\n",
    "end_time=time.time()\n",
    "print(\"It took: %.2f\"%(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
